### Least Squares Regression Properties
+ The sum of  residuals from the least squares regression line is 0, ($\sum$ (y - $\hat{y}$ ) =0 )
+ The sum of squares of residuals from the least squares regression line is a minimum ($\sum$ ($\hat{y}$ - y)$^2$)
+ The simple regression line always passes through the mean of the y variable and the mean of the x variable.
+ The least squares regression coefficients are unbiased estimate of $\beta_0$ and $\beta_1$
### Explaned and Unexplained Of Variations
+ Total Variation is made up of two parts: SST = SSE + SSR
  - SST: Total Sum of Squares   $\sum$ ( y- $\bar{y}$ )$^2$
  - SSE: Sum of Squares Errors   $\sum$ ( y - $\hat{y}$ } )$^2$
  - SSR: Sum of Squares Regression   $\sum$ ( $\hat{y}$ - $\bar{y}$ )$^2$
+ coefficient $\beta_1$ = cor(x)*s$_x$/sy$_y$

### Coefficient of Determination
+ the coefficient of determination is the portion of the total variation in the dependent variable that is explained by the variable in the independent variable.
+ the coefficient of determination is also called the R-squared and is denoted as R$^2$ : 
    R$^2$ = SSR/SST, where 0<= R$^2$ <=1
### Stand Error of Estimate
+ the stand deviation of variation of observations around the regression line is estimate by s$_\xi$  = $\sqrt{\frac{SSE}{n-k-1}}$
### The Standard Deviation of the Regression Slope 
+ The standard error of the regression slope coefficient (b1) is estimated by
   S$_b{_1}$ = $\sqrt{\frac{S\xi}{\Sigma{(x-\bar{x})^2}}}$